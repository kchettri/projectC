#include <stdio.h>
#include <iostream>
#include <fstream>
#include <streambuf>

/* local header, generated by protocol buffers */
#include "hadoop.hdfs/src/fsimage.pb.h"

/* protocol buffer headers */
#include <google/protobuf/io/zero_copy_stream.h>
#include <google/protobuf/io/zero_copy_stream_impl.h>

using namespace std;

typedef unsigned char byte;
typedef unsigned int uint;


struct membuf : std::streambuf
{
    membuf(char* begin, char* end) {
        this->setg(begin, begin, end);
    }
};

//returns integer stored in big endian format
int readInteger(istream& fileReaderStream) {
	unsigned char buffer[4];	
	fileReaderStream.read((char *)buffer, 4);

	return (int) buffer[3] + 
		   ((int) buffer[2] << 8) +
		   ((int) buffer[1] << 16)  + 
		   ((int) buffer[0] << 24); 
}


/* this needs refactoring and change */
bool readDelimitedFrom(
    istream *istream_input,
    google::protobuf::MessageLite* message) {
  // We create a new coded stream for each message.  Don't worry, this is fast,
  // and it makes sure the 64MB total size limit is imposed per-message rather
  // than on the whole stream.  (See the CodedInputStream interface for more
  // info on this limit.)
  google::protobuf::io::ZeroCopyInputStream* raw_input = new google::protobuf::io::IstreamInputStream(istream_input);
  //google::protobuf::io::IstreamInputStream raw_input(istream_input);	

  google::protobuf::io::CodedInputStream input(raw_input);

  //google::protobuf::io::CodedInputStream input(raw_inputInput);

  // Read the size.
  uint32_t size;
  if (!input.ReadVarint32(&size)) return false;

  // Tell the stream not to read beyond that size.
  google::protobuf::io::CodedInputStream::Limit limit =
      input.PushLimit(size);

  // Parse the message.
  if (!message->MergeFromCodedStream(&input)) return false;
  if (!input.ConsumedEntireMessage()) return false;

  // Release the limit.
  input.PopLimit(limit);

  return true;
}


int main(int argc, char* argv[]) {

	GOOGLE_PROTOBUF_VERIFY_VERSION;

	if (argc != 2) {
		cerr << "Usage:  " << argv[0] << " fsimagefile" << endl;
		return -1;
	}
	ifstream imgFile(argv[1], ios::in | ios::binary);

	int fsize = 0;
    fsize = imgFile.tellg();
    imgFile.seekg( 0, std::ios::end );
    fsize = (int)imgFile.tellg() - fsize;

	cout << "File size = " << fsize << endl;
	imgFile.seekg(fsize - 4, std::ios::beg);
	
	cout << "File pointer moved to = " << imgFile.tellg() << endl;

	int fileSummaryLength; 
	fileSummaryLength = readInteger(imgFile);

	cout << "FileSummaryLength=" << fileSummaryLength << endl;
	
	imgFile.seekg(fsize - 4 - fileSummaryLength, std::ios::beg);
	hadoop::hdfs::fsimage::FileSummary fsSummary; 

	readDelimitedFrom(&imgFile, &fsSummary);
	
	if(fsSummary.has_ondiskversion()) {
		cout << "On disk version = " <<  fsSummary.ondiskversion() << endl;
	} else {
		cout << "Does not have ondiskVersion" << endl;
	}

	if(fsSummary.has_layoutversion()) {
		cout << "Layout version = " <<  fsSummary.layoutversion() << endl;
	} else {
		cout << "Does not have layout Version" << endl;
	}

	if(fsSummary.has_codec()) {
		cout << "Codec = " <<  fsSummary.codec() << endl;
	} else {
		cout << "Does not have codec" << endl;
	}

	cout << "Num sections: " << fsSummary.sections_size() << endl;

	for(int i=0; i< fsSummary.sections_size(); i++) {
		hadoop::hdfs::fsimage::FileSummary_Section fsSummarySection = fsSummary.sections(i);
		
		if( fsSummarySection.has_name())  {
			cout << "Section " << i << " name= " << fsSummarySection.name() << endl; 
		}
		
	}

	google::protobuf::ShutdownProtobufLibrary();
	return 0; 

}
